# ğŸ“ gpt
# - gpt (a transformer) takes input tokens and predicts the next token
# - so we need to keep on iterating
# - and that's why it's a cpu intensive task
# - and that's why llms require gpus (for high computing power)
# - because we have to keep running the prediction model

# ğŸ“ google translate used transformer
# - Transformer Architecture was being used heavily in Google Translate
# - this transformer took in an "input sequence" and gave an "output sequence"

# ğŸ“ white paper
# - the word transformer comes from Google's white paper named "Attention is all you need"