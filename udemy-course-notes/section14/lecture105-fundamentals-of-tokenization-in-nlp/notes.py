# üìç tokens and tokenization
# - the definition of a token can vary from model to model
# - for some models, every character can be a token
# - the characters that humans use are mapped to some numbers called tokens
# - this was an oversimplified version of tokenization
# - in reality, this is not the way tokenization works
# - these tokens are sent to the transformer
# - then transformer predicts the next token
# - tokenization : converting user input to some set of numbers understandable by llm
# - we also have detokenization step